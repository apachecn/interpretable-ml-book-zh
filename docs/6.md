# 第六章 基于实例的解释

基于实例的解释方法选择数据集的特定实例来解释机器学习模型的行为或解释底层数据分布。

基于实例的解释大多与模型无关，因为它们使任何机器学习模型都更具可解释性。与模型无关的方法的区别在于，基于实例的方法通过选择数据集的实例来解释模型，而不是通过创建特性摘要(例如特性重要性或部分依赖关系)来解释模型。基于实例的解释只有在我们能够以人类可理解的方式表示数据实例时才有意义。这对图像很有效，因为我们可以直接查看它们。一般来说，如果实例的特征值包含更多的上下文，那么基于实例的方法就可以很好地工作，这意味着数据有一个结构，就像图像或文本一样。以一种有意义的方式表示表格数据更具挑战性，因为一个实例可能包含数百或数千个(较少结构化的)特性。列出所有的特性值来描述一个实例通常是没有用的。如果只有少数几个特性，或者我们有一种方法来总结一个实例，那么它可以很好地工作。

基于实例的解释帮助人类构建机器学习模型的心智模型，以及机器学习模型所训练的数据。它特别有助于理解复杂的数据分布。但是我所说的基于实例的解释是什么意思呢?我们经常在工作和日常生活中使用它们。让我们以一些例子开始。

医生看到病人有不寻常的咳嗽和轻微的发烧。病人的症状让她想起了几年前的另一个病人，她也有类似的症状。她怀疑她现在的病人可能患有同样的疾病，她采集了血液样本来检测这种特殊的疾病。
一位数据科学家正在为他的一位客户做一个新项目:分析导致键盘生产机故障的风险因素。数据科学家记得他所从事的一个类似的项目，并重用了旧项目中的部分代码，因为他认为客户需要相同的分析。
一只小猫坐在失火无人居住的房子的窗台上。消防部门已经到了，其中一名消防队员正在考虑他是否可以冒险进入大楼去救小猫。他还记得自己当消防员时遇到过类似的情况:燃烧缓慢的老木屋往往不稳定，最终倒塌。由于情况相似，他决定不进去，因为房子倒塌的风险太大了。幸运的是，小猫跳出了窗户，安全着陆，没有人在火灾中受伤。快乐的结局。

这些故事用例子或类比来说明我们人类是如何思考的。基于实例的解释的蓝图是:B类似于A, A导致Y，所以我预测B也会导致Y。隐式地，一些机器学习方法是基于实例的。决策树根据对预测目标很重要的特性中的数据点的相似性将数据划分为节点。决策树通过查找相似的实例(=在相同的终端节点中)并返回这些实例结果的平均值作为预测来获得新数据实例的预测。k近邻(knn)方法可以显式地处理基于实例的预测。对于一个新实例，knn模型定位k个最近的邻居(例如k=3个最近的实例)，并返回这些邻居结果的平均值作为预测。knn的预测可以通过返回k个邻居来解释，这同样只有在我们有一个好的方法来表示单个实例时才有意义。

本部分各章包括以下基于实例的解释方法：

•  反事实解释告诉我们，一个实例必须如何改变才能显著地改变它的预测。通过创建反事实实例，我们了解了模型如何进行预测，并能够解释单个预测。

•  对抗性例子是用来愚弄机器学习模型的反事实。重点是推翻预测，而不是解释它。

•  原型是从数据中选择具有代表性的实例，而批评则是这些原型不能很好地代表的实例。

•  影响实例是对预测模型的参数或预测本身影响最大的训练数据点。识别和分析有影响的实例有助于发现数据中的问题，调试模型并更好地理解模型的行为。

•  k近邻模型:一种基于实例的(可解释的)机器学习模型。

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image001.gif)

\1.    Aamodt、Agnar 和 Enric 广场。“基于案例的推理：基础问题、方法变化和系统方法。”人工智能通信 7.1（1994）：39-59。

\2.    Kim, Been, Rajiv Khanna和Oluwasanmi O. Koyejo。“光举例子是不够的，要学会批评!”可解释性的批评。神经信息处理系统的进展(2016)。

6.1 反事实解释

反事实的解释描述了这样一种因果关系:“如果X没有发生，Y也不会发生”。例如:“如果我没有喝一口热咖啡，我的舌头就不会灼伤了。”事情是我烫伤了我的舌头;因为我喝了一杯热咖啡。思考反事实需要想象一个与观察到的事实相矛盾的假设现实(例如，一个我没有喝热咖啡的世界)，因此得名“反事实”。与其他动物相比，思考反事实的能力使我们人类如此聪明。

在可解释的机器学习中，反事实解释可用于解释单个实例的预测。“事件”是一个实例的预测结果，“原因”是这个实例被输入到模型并“引起”某个预测的特定特征值。以图的形式显示，输入和预测之间的关系非常简单:特征值导致预测。

图 6.1：机器学习模型输入与预测之间的因果关系，当模型仅被视为一个黑匣子时。输入导致预测（不一定反映数据的真正因果关系）。

即使在现实中，输入和预测结果之间的关系可能不是因果关系，我们可以把模型的输入看作是预测的原因。

有了这个简单的图表，很容易看出我们如何模拟机器学习模型预测的反事实:我们只是在做出预测之前改变实例的特征值，然后分析预测是如何变化的。我们感兴趣的是预测以相关方式变化的场景，比如预测类的翻转(例如接受或拒绝信贷申请)，或者预测达到某个阈值(例如癌症的概率达到10%)。预测的反事实解释描述了将预测更改为预定义输出的特征值的最小更改。

反事实解释方法与模型无关，因为它只对模型输入和输出有效。这种方法在与模型无关的章节中也很适用，因为解释可以表示为特征值差异的总结(“更改特征a和B以更改预测”)。但是反事实的解释本身就是一个新的实例，所以它存在于本章中(“从实例X开始，改变a和B以获得一个反事实的实例”)。与原型不同，反事实不必是来自训练数据的实际实例，但可以是特性值的新组合。


在讨论如何创建反事实之前，我想讨论一下反事实的一些用例，以及一个好的反事实解释是怎样的。



在第一个例子中，Peter申请了一笔贷款，但是被(机器学习支持的)银行软件拒绝了。他想知道为什么他的申请被拒绝了，他怎样才能提高获得贷款的机会。“为什么”这个问题可以表述为一个反事实:将预测从被拒绝变为被批准的特征(收入、信用卡数量、年龄……)的最小变化是什么?一个可能的答案是:如果彼得每年能多挣1万欧元，他就能得到贷款。或者如果彼得的信用卡少一些，5年前没有拖欠贷款，他就能得到贷款。彼得永远不会知道拒绝的原因，因为银行对透明度没有兴趣，但那是另一个故事。



在我们的第二个例子中，我们想要解释一个模型，它用反事实的解释来预测一个连续的结果。Anna想把她的公寓租出去，但是她不知道要多少钱，所以她决定训练一个机器学习模型来预测租金。当然，因为安娜是一个数据科学家，这就是她解决问题的方式。在输入了所有关于尺寸、位置、是否允许养宠物等细节后，模特告诉她可以收取900欧元。她的期望是1000欧元或更多，但她相信她的模型，并决定发挥公寓的功能价值，看看她如何能提高公寓的价值。她发现，如果公寓再大15平方米，租金超过1000欧元就可以租出去。有趣，但不可行的知识，因为她不能扩大她的公寓。最后，通过调整她控制下的功能值(内置厨房是/否，宠物允许是/否，地板类型等)，她发现如果她允许宠物安装隔热更好的窗户，她可以收取1000欧元。安娜凭直觉与反事实合作来改变结果。



反事实是对人类友好的解释，因为它们与当前实例相反，而且它们是有选择性的，这意味着它们通常只关注少量的特性更改。但反事实受到“罗生门效应”的影响。《罗生门》是一部日本电影，讲述了不同的人对一个武士的谋杀。每个故事都很好地解释了结果，但故事之间相互矛盾。反事实也会发生同样的情况，因为通常会有多种不同的反事实解释。每一个反事实都讲述了一个关于如何达到某种结果的不同“故事”。一个反事实的人可能会说改变特征A，另一个反事实的人可能会说保持A不变但改变特征B，这是一个矛盾。这个多重事实的问题既可以通过报告所有反事实的解释来解决，也可以通过制定一个标准来评估反事实并选择最佳的一个。



说到标准，我们如何定义一个好的反事实的解释?首先，反事实解释的用户在实例的预测中定义了一个相关的更改(=可选的现实)，所以第一个明显的要求是，反事实实例尽可能紧密地产生预定义的预测。不可能总是精确地匹配预定义的输出。在一个有两个类，一个罕见类和一个频繁类的分类设置中，模型总是可以将一个实例分类为频繁类。更改特性值以使预测的标签从普通类转换为罕见类可能是不可能的。因此，我们希望放宽反事实的预测输出必须与定义的结果完全对应的要求。在分类示例中，我们可以寻找一个反事实，即稀有类的预测概率增加到10%，而不是当前的2%。接下来的问题是，对特征的最小变化是什么，使得预测概率从2%变化到10%(或接近10%)?另一个质量标准是，关于特征值，反事实应该尽可能与实例相似。这需要在两个实例之间测量距离。反事实不仅应该接近原始实例，而且应该尽可能少地改变特征。这可以通过选择适当的距离度量来实现，比如曼哈顿距离。最后一个要求是反事实实例应该具有可能的特性值。对于租房的例子，如果公寓的大小是负的，或者房间的数量设置为200，那么产生反事实的解释是没有意义的。根据数据的联合分布，反事实的可能性更大，例如10间20平米的公寓不应被视为反事实的解释。

6.1.1 产生反事实解释

一种简单而天真的反事实解释方法是通过反复试验来寻找。这种方法涉及到随机改变感兴趣实例的特征值，并在预期输出时停止。比如安娜想找一套她可以收取更高租金的公寓。但是有比反复试验更好的方法。首先，我们定义了一个损失函数，该函数以兴趣实例、反事实结果和期望(反事实)结果作为输入。损失度量反事实的预测结果与预定义结果之间的距离，以及反事实与兴趣实例之间的距离。我们可以使用优化算法直接优化损失，也可以通过搜索实例来优化损失，如“Growing Spheres”方法所建议的那样(参见软件和备选方案)。

在本节中，我将介绍Wachter等人(2017)49提出的方法。他们建议尽量减少随之而来的损失。

\[L（x，x^ \ prime，y^ \ prime，lambda）=\lambda\cdot（\hat f（x^ \ prime）-y^ \ prime）^2+d（x，x^ \ prime）\]

第一项是模型预测反事实 x'和期望结果 y'之间的二次距离，用户必须提前定义。第二个术语是要解释的实例 x 和反事实 x'之间的距离 d’，但稍后会详细介绍。参数\（\lambda\）平衡预测中的距离（第一项）和特征值中的距离（第二项）。对给定的\（\lambda\）求解损失，并返回反事实 x'。较高的值\（\lambda\）意味着我们更喜欢接近期望结果 y 的反事实，较低的值意味着我们更喜欢特征值中与 x 非常相似的反事实 x。如果\（\lambda\）非常大，无论预测距离 x 多远，都将选择预测最接近 y'的实例。最终，用户必须决定如何平衡反事实预测与所需结果匹配的要求。t 反事实类似于 x。该方法的作者建议不要为\（\lambda\）选择一个值，而是选择一个公差\（\epsilon\）来决定反事实实例的预测距离 y'的距离。这个约束可以写为：

\[\hat f（x ^ \prime）-y ^ \prime \leq \epsilon \]

为了最小化这个损失函数，可以使用任何合适的优化算法，例如 Nelder Mead。如果您可以访问机器学习模型的渐变，那么您可以使用基于渐变的方法，如 Adam。要解释的实例 x、所需的输出 y'和公差参数\（\epsilon\）必须提前设置。对于 x'和（局部）最佳反事实 x'的损失函数最小化，同时增加\（\lambda\）直到找到足够接近的解（=在公差参数内）。

\[\arg\min_x^ \ prime \ max \ lambda l（x，x^ \ prime，y^ \ prime，lambda）\]

用于测量实例 x 和反事实 x'之间距离的函数 d 是曼哈顿距离加权特征，具有反向中值绝对偏差（MAD）。

\[d（x，x^ \ prime）=\sum j=1 ^p \ frac x j-x ^ \ prime mad j \]

总距离是所有 P 向特征距离的总和，即实例 x 和反事实 x'之间特征值的绝对差异。特征距离通过特征 j 在数据集上的中值绝对偏差的倒数缩放，定义为：

\[mad_j=\text 中位数 1，ldots，n（x i，j-\text 中位数 1，ldots，n（x l，j）]]

一个向量的中值是向量的一半大于另一半小于的值。MAD等价于一个特征的方差，但我们不是以均值为中心，对平方距离求和，而是以中位数为中心，对绝对距离求和。提出的距离函数比欧几里得距离具有引入稀疏性的优点。这意味着当不同的特征较少时，两个点之间的距离更近。它对异常值更健壮。为了使所有的功能都达到相同的比例，按MAD的比例缩放是必要的——不管你是按平方米还是平方英尺来测量公寓的大小，这都不重要。

产生反事实的方法很简单:

\1.    选择要解释的实例 x、所需的结果 y、公差（epsilon）和（low）初始值（lambda）。

\2.    抽样一个随机的实例作为初始反事实。

\3.    以初始抽样反事实为出发点，对损失进行优化。

\4.    而\（\hat f（x ^ \ prime）-y ^ \ prime>\epsilon\）：

增加\（\lambda\）。

以当前反事实为起点优化损失。退回减少损失的反事实。

\5.    重复步骤 2-4 并返回反事实列表或最小化损失的列表。

6.1.2 示例

这两个例子都来自Wachter et. al(2017)的著作。

在第一个例子中，作者训练了一个三层的全连接神经网络来预测一个学生在法学院的第一年的平均成绩，基于在法学院之前的平均绩点(GPA)，种族和法学院入学考试分数。目标是为每个回答以下问题的学生找到反事实的解释:如何改变输入特性才能得到预期的0分?由于之前已经进行了标准化，所以0分的学生和一般学生一样好。负分数表示低于平均水平，正分数表示高于平均水平。

下表显示了所学的反事实:

## ScoregpalSatracegpa x'Lsat x'race x'

| 0.17 3.1 条  | 39.0 分 | 第 3.1 条 | 34.0 条   | 0    |
| ----------- | ------ | ------- | -------- | ---- |
| 0.54 3.7 条  | 48.0 分 | 第 3.7 条 | 第 32.4 条 | 0    |
| -0.77 3.3 条 | 28.0 1 | 第 3.3 条 | 33.5 条   | 0    |
| -0.83 2.4 条 | 28.5 一 | 第 2.4 条 | 35.8 条   | 0    |
| -0.57 2.7 条 | 18.3 0 | 2.7 条   | 第 34.9 条 | 0    |

第一列包含预测的得分，接下来的3列是原始的特征值，最后的3列是反事实的特征值，结果得分接近于0。前两排是预测高于平均水平的学生，其他三排低于平均水平。前两行的反事实描述了如何改变学生的特征以降低预期的分数，而其他三种情况则描述了如何改变特征以使分数提高到平均水平。增加分数的反事实总是将种族从黑色(编码为1)更改为白色(编码为0)，这显示了模型的种族偏见。GPA在反事实中没有变化，但LSAT有变化。

第二个例子显示了对预测糖尿病风险的反事实解释。一个三层的完全连接的神经网络被训练来预测糖尿病的风险取决于年龄，身体质量指数，怀孕次数等等。反事实回答了这个问题:哪些特征值必须改变才能增加或减少糖尿病的风险评分到0.5?发现了下列反事实:

•  人 1：如果你的 2 小时血清胰岛素水平为 154.3，你将得到 0.51 分。

•  人 2：如果你的 2 小时血清胰岛素水平为 169.5，你将得到 0.51 分。

•  人 3：如果你的血糖浓度是 158.3，你的 2 小时血清胰岛素水平是 160.5，你将得到 0.51 分。

6.1.3 优势

反事实解释的解释是非常清楚的。如果实例的特征值根据反事实更改，则预测更改为预定义的预测。这里没有额外的假设，也没有魔法的背景。这也意味着它不像LIME那样危险，因为我们不清楚我们能在多大程度上推断出当地的解释模型。

反事实方法创建一个新实例，但是我们也可以通过报告哪些特性值已经更改来总结反事实。这为我们提供了两个报告结果的选项。您可以报告反事实实例，或者突出显示感兴趣的实例和反事实实例之间更改了哪些特性。

反事实方法不需要访问数据或模型。它只需要访问模型的预测函数，例如，该函数也可以通过web API工作。这对于那些由第三方审计或向用户提供解释而不披露模型或数据的公司很有吸引力。由于商业秘密或数据保护的原因，公司有保护模型和数据的利益。反事实解释在解释模型预测和保护模型所有者的利益之间提供了一种平衡。
这种方法也适用于不使用机器学习的系统。我们可以为任何接收输入并返回输出的系统创建反事实。预测公寓租金的系统也可能由手写的规则组成，而反事实的解释仍然有效。

反事实解释方法相对容易实现，因为它本质上是一个损失函数，可以用标准的优化器库进行优化。必须考虑一些额外的细节，比如将功能值限制在有意义的范围内(例如，只有正的公寓大小)。

6.1.4 缺点

对于每一个实例，你通常会发现多个反事实的解释(罗生门效应)。这是不方便的——大多数人更喜欢简单的解释而不是真实世界的复杂性。这也是一个现实的挑战。假设我们为一个例子生成了23个反事实的解释。我们都报告了吗?只有最好的吗?如果它们都相对“好”，但又非常不同呢?对于每个项目，必须重新回答这些问题。拥有多个反事实的解释也可能是有利的，因为这样人类可以选择那些与他们先前的知识相对应的解释。

发现了一个反事实的实例——对于给定的公差没有保证ϵ。这并不一定是方法的错误，而是取决于数据。

该方法不能很好地处理多个层次的分类特征。该方法的作者建议对分类特征的每个特征值的组合分别运行该方法，但是如果有多个具有多个值的分类特征，则会导致组合爆炸。例如，6个分类特征和10个独特的关卡意味着100万次运行。Martens等人(2014)提出了一种仅针对分类特征的解决方案。在Python包Alibi中实现了一个解决方案，该解决方案同时处理数值变量和分类变量，并有原则地为分类变量生成扰动。

6.1.5 软件和备选方案
反事实解释在Python包的Alibi中实现。包的作者实现了一个简单的反事实方法和一个扩展的方法，该方法使用类原型来提高算法输出的可解释性和收敛性。

Martens等(2014)提出了一种非常类似的方法来解释文档分类。在他们的工作中，他们专注于解释为什么一个文件是或不是一个特定的类别。与本章提出的方法不同的是，Martens等(2014)关注文本分类器，它将单词出现作为输入。

另一种搜索反事实的方法是Laugel et. al(2017)52的Growing Spheres算法。该方法首先在感兴趣点周围绘制一个球体，对球体内的点进行采样，检查其中一个采样点是否产生所需的预测，并相应地收缩或扩展球体，直到找到一个(稀疏的)反事实并最终返回。他们在论文中没有使用“反事实”这个词，但方法非常相似。他们还定义了一个支持反事实的损失函数，其特征值的变化越少越好。他们不是直接优化函数，而是建议使用球体进行上述搜索。

图6.2:Laugel等人(2017)对球体生长和稀疏反事实选择的说明。

Ribeiro et. al(2018)53的主播是反事实的对立面。锚回答了以下问题:哪些特征足以锚定预测，即改变其他特征不能改变预测?一旦我们发现了作为预测锚点的特征，我们将不再通过改变锚点中未使用的特征来发现反事实的实例。

图 6.3:Ribeiro 等人（2018 年）的锚示例。

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.gif)

\1.    瓦赫特、桑德拉、布伦特·米特斯塔特和克里斯·拉塞尔。“不打开黑匣子的反事实解释：自动决策和 GDPR。”（2017 年）。

\2.    马丁斯、大卫和福斯特教务长。“解释数据驱动文档分类。”（2014 年）。

[↩](https://christophm.github.io/interpretable-ml-book/counterfactual.html#fnref46)

\3.    Laugel，Thibault 等人“机械学习中基于比较的可解释性的逆向分类”，ARXIV 预印 ARXIV:1712.08443（2017）。

\4.    Ribeiro、Marco Tulio、Sameer Singh 和 Carlos Guestrin。“锚：高精度模型不可知论解释”，AAAI 人工智能会议（2018）。

6.2 对抗性示例

一个对抗性的例子是一个带有小的、故意的特征扰动的实例，这些扰动会导致机器学习模型做出错误的预测。我建议先阅读关于反事实解释的章节，因为概念非常相似。对抗性的例子是反事实的例子，目的是欺骗模型，而不是解释它。

为什么我们对对抗性的例子感兴趣?它们难道不只是机器学习模型的有趣副产品，没有实际意义吗?答案显然是“不”。对抗性示例使机器学习模型容易受到攻击，如下面的场景所示。

一辆自动驾驶汽车因为无视停车标志而与另一辆汽车相撞。有人在这个标志上放了一张图片，它看起来像一个停车标志，上面有一点人类的泥土，但它的设计看起来像一个禁止停车的标志，是为汽车的标志识别软件设计的。

垃圾邮件检测器无法将电子邮件归类为垃圾邮件。垃圾邮件被设计成类似于普通的电子邮件，但目的是欺骗收件人。

一台机器学习扫描仪在机场扫描手提箱，寻找武器。为了避免被发现，人们发明了一种刀具，让系统认为它是一把雨伞。

让我们来看看一些创建对抗性例子的方法。

6.2.1 方法和示例

创建对抗性示例有许多技巧。大多数方法建议最小化对抗性示例和要操纵的实例之间的距离，同时将预测转换为期望的(对抗性的)结果。一些方法需要访问模型的梯度，当然，这只适用于基于梯度的模型，如神经网络，其他方法只需要访问预测函数，这使得这些方法模型不可知。本节的方法主要针对具有深度神经网络的图像分类器，因为在这方面已经做了大量的研究，并且对敌图像的可视化具有很强的教育意义。图像的反例是在应用过程中有意干扰像素以欺骗模型的图像。这些例子令人印象深刻地表明，用于物体识别的深度神经网络很容易被看似对人类无害的图像所欺骗。如果你还没有看过这些例子，你可能会感到惊讶，因为预测的变化对于一个人类观察者来说是不可理解的。对抗性的例子就像光学幻像，不过是机器的幻像。

## 我的狗出问题了

Szegedy等人(2013)54在他们的著作《神经网络的有趣特性》中使用了一种基于梯度的优化方法来寻找深度神经网络的对抗性例子。

图 6.4:Szegedy 等人（2013 年）对 Alexnet 的敌对示例。左列中的所有图像都已正确分类。中间一列显示添加到图像中的（放大的）错误，以生成右一列中的图像，所有分类（错误）为“鸵鸟”。

这些相反的例子是通过最小化与 r 有关的以下函数而产生的：

\[损失（\hat f（x+r），l）+c\cdot r \]

在这个公式,x是一个图像(像素)表示为一个向量,r是像素的变化来创建一个敌对的形象(x + r产生一个新的图像),l是期望的结果类,参数c是用来平衡之间的距离图像和预测之间的距离。第一项是对抗性样本预测结果与期望类l之间的距离，第二项是对抗性样本与原始图像之间的距离。这个公式几乎与损失函数相同，可以产生反事实的解释。对于r有额外的约束，因此像素值保持在0和1之间。作者建议用一个盒子约束的L-BFGS来解决这个优化问题，这是一种使用梯度的优化算法。

## 扰动熊猫：快速梯度标记法

Goodfellow 等人（2014 年发明了用于生成对抗图像的快速梯度符号方法。

梯度符号法利用底层模型的梯度来寻找对立的例子。

原始图像 X 是通过在每个像素上加上或减去一个小的错误来操作的。我们加或减\（\epsilon\）取决于像素的渐变符号是正的还是负的。在梯度方向上增加误差意味着图像被有意修改，从而导致模型分类失败。

图 6.5:Goodfellow 等人（2014 年）在神经网络中使熊猫看起来像长臂猿。通过将小扰动（中间图像）添加到原始熊猫像素（左图像）中，作者创建了一个敌对的例子，它被归类为长臂猿（右图像），但在人类看来像熊猫。

下面的公式描述了快速梯度符号方法的核心内容：

其中\（\bigstriangledown_x j\）是模型损失函数相对于原始输入像素向量 x 的梯度，y 是 x 的真标签向量，而\（\theta\）是模型参数向量。从梯度向量（与输入像素的向量一样长）我们只需要符号：如果像素强度增加损失（模型产生的误差），梯度符号为正（+1），如果像素强度减少损失，则为负（-1）。当神经网络线性处理输入像素强度和类分数之间的关系时，就会出现此漏洞。特别是，倾向于线性化的神经网络结构，如 LSTMS、MAXOUT 网络、带有 RELU 激活单元的网络或其他线性机器学习算法（如逻辑回归）易受梯度符号法的影响。攻击是通过外推法进行的。输入像素强度和类分数之间的线性关系导致易受异常值的影响，即通过将像素值移动到数据分布之外的区域，可以欺骗模型。我期望这些对抗性的例子对于给定的神经网络体系结构是非常具体的。但事实证明，你可以重复使用敌对的例子来欺骗在同一个任务上训练了不同体系结构的网络。

Goodfellow 等人（2014）建议在培训数据中添加对抗性示例，以学习健壮的模型。

## 水母……不，等等。浴缸：1 像素攻击

Goodfellow 及其同事（2014 年）提出的方法需要改变很多像素，即使只是稍微改变一点。但是如果你只能改变一个像素呢？你能欺骗一个机器学习模型吗？Su 等人（2019 年）表明，通过改变单个像素实际上可以欺骗图像分类器。

图 6.6：通过故意改变一个像素（用圆圈标记），在 ImageNet 上训练的神经网络被欺骗，从而预测错误的类而不是原始类。Su 等人（2019）的研究。

与反事实类似，1像素攻击查找修改后的示例x '，它与原始图像x接近，但将预测更改为对抗性结果。然而，亲密度的定义是不同的:只有一个像素可能会改变。1像素攻击使用差分进化来找出要改变哪个像素以及如何改变。差异进化从某种程度上是受物种生物进化的启发。被称为候选解决方案的一群个体一代一代地重新组合，直到找到一个解决方案。每个候选解决方案编码一个像素修改，并由五个元素组成的向量表示:x和y坐标以及红、绿、蓝(RGB)值。例如，搜索从400个候选解决方案(=像素修改建议)开始，并使用以下公式从父代创建新一代候选解决方案(子):

\[X I（G+1）=X R1（G）+F\CDOT（X R2（G）+X R3（G））]

其中每个（x_i）是候选解的元素（x 坐标、y 坐标、红色、绿色或蓝色），g 是当前生成的，f 是缩放参数（设置为 0.5），r1、r2 和 r3 是不同的随机数。每个新的子候选解决方案依次是一个像素，其中包含五个位置和颜色属性，每个属性都是三个随机父像素的混合。

如果一个候选解决方案是一个对立的示例，意味着它被分类为不正确的类，或者达到了用户指定的最大迭代次数，则停止创建子级。

## 一切都是一台烤面包机：敌对的补丁

我最喜欢的方法之一是将对抗性的例子带入物理现实。Brown et. al(2017)57设计了一个可打印的标签，可以贴在物体旁边，使它们看起来像一个图像分类器的烤面包机。出色的工作!

图 6.7：一个贴纸，它使在 ImageNet 上训练的 VGG16 分类器将香蕉的图像分类为烤面包机。Brown 等人的工作（2017 年）。

这种方法不同于目前所提出的对抗性例子的方法，因为它消除了对抗性图像必须非常接近原始图像的限制。取而代之的是，该方法将图像的一部分完全替换为可以呈现任何形状的补丁。patch的图像是在不同的背景图像上进行优化的，patch在图像上的位置是不同的，有时移动，有时变大或者变小，然后旋转，所以patch可以在很多情况下工作。最后，将优化后的图像打印出来，用于欺骗野外的图像分类器。

## 千万不要把 3D 打印的乌龟带到枪战中——即使你的电脑认为这是个好主意：强大的对抗性例子

下一种方法是在烤面包机上添加另一个维度：Athalye 等人（2017）3 印刷了一只乌龟，从几乎所有可能的角度来看，它看起来就像一把步枪。是的，你读对了。对人类来说，一个看起来像乌龟的物体在计算机上看起来就像一把步枪！

图 6.8：一只 3D 打印乌龟，被 TensorFlow 的标准预先训练识别为步枪。

inceptionv3 分类器。Athalye 等人的工作（2017 年）

作者发现了一种方法，可以在3D中为2D分类器创建一个对抗性的例子，这个例子在转换方面是对抗性的，比如所有可能的旋转乌龟、放大等等。其他方法，如快速梯度方法不再工作时，图像旋转或视角变化。Athalye et. al(2017)提出了期望超越变换(expect Over Transformation, EOT)算法，这是一种生成对抗性示例的方法，即使在图像被变换时也能工作。EOT背后的主要思想是在许多可能的转换中优化对抗性示例。EOT并没有最小化反例与原始图像之间的距离，而是将两者之间的期望距离保持在某个阈值以下(给定一个选定的可能的转换分布)。变换后的期望距离可表示为:

\[\mathbb e t\sim t[d（t（x^\prime），t（x））]\

其中 x 是原始图像，t（x）是转换后的图像（例如旋转），x'是敌对的例子，t（x'）是转换后的版本。除了处理转换的分布之外，eot 方法还遵循熟悉的模式，将搜索敌对示例作为优化问题进行框架化。我们试图找到一个对抗性的例子 x'来最大化所选类（y_t）的概率（例如“来复枪”）在可能的转换分布 t：

\[\arg\max_x ^ \prime \mathbb e t \sim t[日志 p（y_t t（x ^ \prime））]\]

在限制条件下，敌对示例 x'和原始图像 x 之间所有可能转换的预期距离保持在某个阈值以下：

\[\mathbb e t\sim t[d（t（x^\prime），t（x））]<\epsilon\quad\text and \quad x \in[0,1]^d\]

我认为我们应该关注这种方法所带来的可能性。另一种方法是基于数字图像的处理。然而，这些3d打印的、健壮的对抗性示例可以插入任何真实场景，并欺骗计算机错误地对对象进行分类。让我们回过头来看:如果有人发明了一种看起来像乌龟的步枪呢?

## 蒙蔽的对手：黑匣子攻击

想象一下下面的场景:我让您通过Web API访问我的优秀图像分类器。您可以从模型中获得预测，但是您不能访问模型参数。从您的沙发方便，您可以发送数据和我的服务回答与相应的分类。大多数对抗性攻击在这种情况下并不适用，因为它们需要访问底层深度神经网络的梯度来找到对抗性的例子。Papernot和同事(2017)59表明，在没有内部模型信息和无法访问训练数据的情况下，可以创建对抗性示例。这种(几乎)零知识攻击称为黑箱攻击。

工作原理：

\1.    从一些与训练数据来自同一域的图像开始，例如，如果要攻击的分类器是数字分类器，则使用数字图像。需要了解该领域的知识，但不需要访问培训数据。

\2.    从黑盒中获取当前图像集的预测。

\3.    在当前图像集（例如神经网络）上训练代理模型。

\4.    使用启发式方法创建一组新的合成图像，该方法检查当前图像集，在哪个方向上操作像素以使模型输出具有更大的方差。

\5.    对预定义的 epoch 数重复步骤 2 到 4。

\6.    使用快速梯度法（或相似法）为代理模型创建对抗性示例。

\7.    用敌对的例子攻击原始模型。

代理模型的目的是近似黑盒模型的决策边界，但不一定达到同样的精度。

作者通过攻击在各种云机器学习服务上训练的图像分类器来测试这种方法。这些服务根据用户上传的图像和标签训练图像分类器。该软件自动训练模型——有时使用用户不知道的算法——并部署它。分类器然后对上传的图像进行预测，但是模型本身不能被检查或下载。作者能够为不同的提供者找到对抗性的例子，多达84%的对抗性的例子被错误分类。

即使要欺骗的黑盒模型不是神经网络，该方法也能起作用。这包括没有梯度的机器学习模型，如决策树。

6.2.2 网络安全观

机器学习处理已知的未知:从已知的分布中预测未知的数据点。攻击防御处理的是未知的未知:从未知的对抗输入分布中稳健地预测未知的数据点。随着机器学习被集成到越来越多的系统中，如自动驾驶汽车或医疗设备，它们也成为攻击的切入点。即使机器学习模型对测试数据集的预测是100%正确的，也可以找到反面的例子来欺骗模型。机器学习模型防御网络攻击是网络安全领域的一个新领域。

Biggio et. al(2018)60对十年来关于对抗性机器学习的研究进行了很好的回顾，本节正是基于此。网络安全是一场军备竞赛，在这场竞赛中，攻击者和防御者一次又一次地智胜对方。

网络安全有三条黄金法则:1)了解你的对手2)积极主动3)保护你自己。

不同的应用程序有不同的对手。试图通过电子邮件诈骗他人钱财的人是电子邮件服务的用户和提供者的对手代理。提供商想要保护他们的用户，这样他们就可以继续使用他们的邮件程序，攻击者想要让人们给他们钱。了解你的对手意味着了解他们的目标。假设你不知道这些垃圾邮件的存在，而唯一滥用电子邮件服务的是发送盗版音乐，那么辩护将是不同的(例如，扫描附件的版权材料，而不是分析文本的垃圾邮件指标)。

主动意味着积极地测试和识别系统的弱点。当你主动尝试用对抗的例子来欺骗模型，然后对它们进行防御时，你就是主动的。使用解释方法来了解哪些特征是重要的，以及这些特征如何影响预测，也是了解机器学习模型弱点的一个主动步骤。作为一名数据科学家，在这个危险的世界里，你相信你的模型吗?您是否分析了模型在不同场景中的行为，确定了最重要的输入，检查了一些示例的预测解释?你有没有试过寻找对抗性输入?机器学习模型的可解释性在网络安全中起着重要作用。被动的，与主动的相反，意味着等待直到系统被攻击，然后才理解问题并安装一些防御措施。

我们如何保护我们的机器学习系统不受敌对例子的影响?一种主动的方法是用对抗性的例子对分类器进行迭代再训练，也称为对抗性训练。其他方法基于博弈论，如学习特征的不变变换或鲁棒优化(正则化)。另一种建议的方法是使用多个分类器，而不是一个，并让它们对预测进行投票(集成)，但这并不能保证有效，因为它们都可能遇到类似的对抗性示例。另一种效果不佳的方法是梯度掩蔽，它通过使用最近邻分类器而不是原始模型来构造一个没有有用梯度的模型。

我们可以通过攻击者对系统的了解程度来区分攻击的类型。攻击者可能拥有完美的知识(白盒攻击)，这意味着他们知道关于模型的一切，比如模型的类型、参数和训练数据;攻击者可能有部分知识(灰箱攻击)，这意味着他们可能只知道所使用的特征表示和模型类型，但无法访问训练数据或参数;攻击者可能没有知识(黑盒攻击)，这意味着他们只能以黑盒方式查询模型，而不能访问训练数据或关于模型参数的信息。根据信息的级别，攻击者可以使用不同的技术来攻击模型。正如我们在示例中看到的，即使在黑箱案例中，也可以创建对抗性的示例，因此隐藏关于数据和模型的信息不足以抵御攻击。

鉴于攻击者和防御者之间的猫捉老鼠游戏的性质，我们将在这一领域看到许多发展和创新。想想那些不断演变的垃圾邮件吧。针对机器学习模型提出了新的攻击方法，并针对这些新的攻击提出了新的防御措施。更强大的攻击被开发，以逃避最新的防御，等等，无穷无尽。在这一章中，我希望让你对对抗性例子的问题更加敏感，并且只有通过前瞻性地研究机器学习模型，我们才能发现和弥补弱点。

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image003.gif)

\1.    Szegedy，Christian 等人“神经网络的有趣特性”，arxiv 预印 arxiv:1312.6199（2013）。

\2.    Goodfellow、Ian J、Jonathon Shlens 和 Christian Szegedy。“解释和利用个人示例”，arxiv 预印 arxiv:1412.6572（2014）。

\3.    苏、加威、达尼洛·瓦斯卡塞洛斯·瓦加斯和樱井。“愚弄深度神经网络的单像素攻击”，《IEEE 进化计算汇刊》（2019 年）。

\4.    Brown，Tom B.等人“对抗补丁。”arxiv 预印 arxiv:1712.09665（2017）。

\5.    阿塔利、安尼和伊利亚·萨茨基弗。“合成强大的对抗性示例”，arxiv 预印 arxiv:1707.07397（2017）。

\6.    Papernot，Nicolas 等人“针对机器学习的实际黑盒攻击”，2017 年 ACM 亚洲计算机与通信安全会议进程。ACM（2017 年）。

\7.    Biggio、Battista 和 Fabio Roli。“野性模式：对抗性机器学习兴起十年后”，《模式识别》84（2018）：317-331。

6.3 原型和批评

原型是代表所有数据的数据实例。批评是一组原型不能很好地表示的数据实例。批评的目的是与原型一起提供见解，特别是对于原型不能很好地表示的数据点。原型和批评可以独立于机器学习模型用于描述数据，但它们也可以用于创建可解释模型或使黑箱模型可解释。

在本章中，我使用“数据点”这个表达式来指代单个实例，强调实例也是坐标系统中的一个点，其中每个特征都是一个维度。下图显示了一个模拟的数据分布，其中一些实例被选择为原型，另一些则作为批评。小点是数据，大点是原型，大点是批评。原型是(手动)选择的，以覆盖数据分布的中心，批评是没有原型的集群中的点。原型和批评总是来自数据的实际实例。

图 6.9：具有两个特性 x1 和 x2 的数据分布的原型和批评。

我手动选择了原型，这并不能很好地扩展，可能会导致糟糕的结果。有许多方法可以在数据中找到原型。其中之一是k-medoids，这是一种与k-means算法相关的聚类算法。任何将实际数据点作为集群中心返回的集群算法都有资格选择原型。但是这些方法大多只找到原型，没有批评。本章介绍Kim et. al(2016)61的mmd - criticism，该方法将原型和批评结合在一个单一的框架中。

mmd -批评家比较数据的分布和选择的原型的分布。这是理解mmd -批评家方法的中心概念。mdd -批评家选择最小化两个发行版之间差异的原型。高密度区域的数据点是很好的原型，特别是从不同的“数据簇”中选择数据点时。来自原型不能很好解释的区域的数据点被选择为批评。

让我们更深入地研究这个理论。

6.3.1 理论

可以简要总结高层次的 MMD 批评程序：

\1.    选择要查找的原型和批评的数量。

\2.    用贪婪的搜索寻找原型。选择原型是为了使原型的分布接近数据分布。

\3.    贪婪地寻找批评。当原型的分布不同于数据的分布时，选择点作为批评。

我们需要一些元素来为一个具有mmd - critics的数据集寻找原型和评论。作为最基本的组成部分，我们需要一个核函数来估计数据密度。内核是根据两个数据点的接近程度对它们进行加权的函数。根据密度估计，我们需要一个度量，它告诉我们两个分布有多么不同，这样我们就可以确定我们选择的原型的分布是否接近数据分布。这是通过测量最大平均偏差(MMD)来解决的。同样基于内核函数，我们需要witness函数来告诉我们在一个特定的数据点上两个分布是如何不同的。有了见证函数，我们可以选择批评，即原型和数据的分布有分歧，见证函数有较大绝对值的数据点。最后一个要素是对好的原型和批评的搜索策略，这是通过简单的贪婪搜索解决的。

让我们从最大平均偏差(MMD)开始，它度量两个分布之间的差异。原型的选择创建了原型的密度分布。我们想要评估原型分布是否不同于数据分布。我们用核密度函数来估计。最大平均差度量两个分布之间的差值，即两个分布的期望差在函数空间上的最大值。都清楚了吗?就我个人而言，当我看到数据是如何计算的时候，我对这些概念的理解要好得多。下面的公式展示了如何计算平方MMD测度(MMD2):

\[mmd^2=\frac 1 m^2 \ sum i，j=1 k（z i，z j）-\frac 2 m n \ sum i，j=1 m，n k（z i，x j）+\frac 1

_n^2 \总和 i，j=1 ^n k（x_i，x_j）]

k是度量两点相似度的核函数，后面会详细介绍。m是原型z的数量，n是原始数据集中的数据点x的数量。原型z是数据点x的选择，每个点都是多维的，也就是说它可以有多个特征。mmd -批评家的目标是最小化MMD2。MMD2越接近于0，原型的分布就越适合数据。MMD2降至零的关键是这个词在中间,计算平均原型和其他所有数据点之间的距离(乘以2)。如果这一项加起来第一项(彼此的平均接近原型)+最后一学期(数据点之间的平均距离),然后原型解释数据。尝试一下如果您使用所有n个数据点作为原型，公式会发生什么变化。

下图说明了MMD2度量。第一个图显示了具有两个特征的数据点，其中数据密度的估计以阴影背景显示。其他每个情节都显示了不同的原型选择，以及情节标题中的MMD2度量。原型是大点，它们的分布以等高线表示。选择最能覆盖这些场景中的数据的原型(左下)的差异值最低。

图 6.10：具有两个特征和不同原型选择的数据集的平方最大平均差异度量（mmd2）。

内核的选择是径向基函数 kernel:\[k（x，x^\prime）=exp\left（\gamma x-x^\prime ^2\right）\]

其中 x-x'是两点之间的欧几里得距离，而\（\gamma\）是一个缩放参数。2.当两点之间的距离无穷远时，内核值随两点之间的距离而减小，范围在 0 和 1 之间：当两点之间无限远时为零；当两点相等时为一。

我们将 mmd2 度量、内核和贪婪搜索结合在一个查找原型的算法中：

•  从一个空的原型列表开始。

•  当原型数量低于所选数量 m 时：

对于数据集中的每个点，检查将该点添加到原型列表时 MMD2 减少了多少。将最小化 mmd2 的数据点添加到列表中。

•  返回原型列表。

找到批评的剩余要素是见证函数，它告诉我们在一个特定点上两个密度估计值有多少不同。可通过以下方式估算：

\[见证人（x）=\frac 1 n \ sum i=1 ^nk（x，x i）-\frac 1 m \ sum j=1 ^mk（x，z j）\]

对于两个数据集(具有相同的特征)，witness函数为您提供了评估x点更适合哪个经验分布的方法。为了找到批评，我们寻找证人功能在消极和积极方向的极端价值。见证函数的第一项是x点与数据的平均接近度，第二项是x点与原型的平均接近度。如果证人功能点x是接近于零,数据和的密度函数原型很近,这意味着原型的分布类似于数据点的分布x。消极的证人x点函数意味着原型分布高估了数据分布(例如,如果我们选择一个原型,但只有一些数据点附近);点x的正见证函数意味着原型分布低估了数据分布(例如，如果x周围有很多数据点，但我们没有选择附近的任何原型)。

为了给您更多的直观感受，让我们使用最低的MMD2重用来自于plot的原型，并显示一些手动选择的点的witness函数。下图中的标签显示了各个标记为三角形的点的witness函数的值。只有中间的点有很高的绝对值，因此是一个很好的批评对象。

图 6.11：不同点见证功能的评估。

witness函数允许我们显式地搜索原型不能很好地表示的数据实例。批评是证人职能中具有较高绝对值的点。像原型一样，批评也是通过贪婪的搜索发现的。但是，我们不是在减少总体的MMD2，而是在寻找最大化包含见证函数和正则化项的成本函数的点。优化函数中的附加项加强了点的多样性，这使得点来自不同的集群。

第二步与如何找到原型无关。我也可以选择一些原型，并使用这里描述的过程来学习批评。或者原型可以来自任何集群过程，比如k-medoid。

这就是mmd -批评家理论的重要部分。还有一个问题:如何将mmd -批评家用于可解释的机器学习?

mdd -批评家可以通过三种方式增加可解释性:帮助更好地理解数据分布;通过建立一个可解释的模型;通过使黑箱模型可解释。

如果您将mmd -批评家应用于您的数据以查找原型和批评，它将提高您对数据的理解，特别是当您有一个带有边缘情况的复杂数据分布时。但是有了mmd -批评家，你可以实现更多!

例如，您可以创建一个可解释的预测模型:一个所谓的“最近原型模型”。预测函数定义为:

\[\hat f（x）=argmax i \ in s k（x，x i）\]

这意味着我们从最接近新数据点的一组原型中选择原型 I，从这个意义上说，它产生了内核函数的最高值。原型本身作为预测的解释返回。此过程有三个调整参数：内核类型、内核缩放参数和原型数量。所有参数都可以在交叉验证循环中进行优化。这种方法不使用批评。

作为第三种选择，我们可以使用 MMD 批评家，通过检查原型和批评以及它们的模型预测，使任何机器学习模型都可以全球解释。程序如下：

\1.    与 MMD 评论家一起寻找原型和评论。

\2.    像往常一样训练机器学习模型。

\3.    用机器学习模型预测原型和批评的结果。

\4.    分析预测：在哪种情况下算法是错误的？现在，您有一些很好地表示数据的示例，帮助您发现机器学习模型的弱点。

这有什么用呢?还记得谷歌的图像分类器将黑人定义为大猩猩的时候吗?也许他们应该在部署图像识别模型之前使用这里描述的过程。仅仅检查模型的性能是不够的，因为如果99%是正确的，这个问题仍然可能在1%中。标签也可能是错的!遍历所有训练数据并执行完整性检查(如果预测有问题)可能会发现问题，但不可行。但是，选择——比方说几千个——原型和批评是可行的，而且可能暴露了数据的一个问题:它可能显示缺乏深色皮肤的人的图像，这表明数据集的多样性存在问题。或者它可以显示一个或多个深色皮肤的人的图片作为原型，或者(可能)作为对臭名昭著的“大猩猩”分类的批评。我不保证mdd -批评家一定会拦截这类错误，但这是一个很好的完整性检查。

6.3.2 示例

我从mmd -批评家的论文中选取了一些例子。这两个应用程序都基于图像数据集。每个图像由2048维的图像嵌入表示。图像嵌入是一个带有数字的向量，它捕获图像的抽象属性。嵌入向量通常从经过训练的神经网络中提取，用于解决图像识别任务，在这种情况下是ImageNet挑战。利用这些嵌入向量计算图像之间的核距离。

第一个数据集包含来自ImageNet数据集的不同犬种。mmd -批评家应用于两个犬种类别的数据。狗在左边，原型通常显示狗的脸，而批评是没有狗的脸或不同的颜色(如黑色和白色)的图像。在右边，原型包含了狗的户外图像。这些批评包括穿着戏服的狗和其他不寻常的情况。

图 6.12：来自 ImageNet 数据集的两种狗品种的原型和批评。

MMD 评论家的另一个例子使用手写数字数据集。

查看实际的原型和批评，您可能会注意到每个数字的图像数量是不同的。这是因为在整个数据集中搜索了固定数量的原型和批评，而不是每个类使用固定数量。正如预期的那样，原型显示了不同的数字写入方式。批评包括线条粗细异常的例子，也包括无法识别的数字。

图 6.13：手写数字数据集的原型和批评。

6.3.3 优势

在一项用户研究中，mmd -批评家的作者给参与者提供了图像，他们必须在视觉上匹配两组图像中的一组，每组图像代表两个类中的一个(例如，两个狗品种)。当展示原型和批评而不是一个类的随机图像时，参与者表现得最好。

您可以自由选择原型和批评的数量。

mmd -批评家处理数据的密度估计。这适用于任何类型的数据和任何类型的机器学习模型。
该算法易于实现。

mdd -批评家在增加可解释性的方式上非常灵活。它可以用来理解复杂的数据分布。它可以用来建立一个可解释的机器学习模型。或者，它可以解释黑箱机器学习模型的决策过程。

发现批评是独立于原型的选择过程的。但是根据mdd -批评家选择原型是有意义的，因为这样一来，原型和评论都是使用比较原型和数据密度的相同方法创建的。

6.3.4 缺点

虽然在数学上，原型和批评的定义是不同的，但它们的区别是基于一个截止值(原型的数量)。假设您选择了数量太少的原型来覆盖数据分布。这些批评最终会出现在那些没有得到很好解释的领域。但是如果你添加更多的原型，它们也会在相同的区域结束。任何解释都必须考虑到批评强烈地依赖于现有的原型和(任意的)原型数量的截止值。

你必须选择原型和批评的数量。虽然这可能是很好的，但它也是一个缺点。我们到底需要多少原型和批评?越多越好?越少越好?一种解决方案是通过测量人们查看图像的时间来选择原型和批评的数量，这取决于特定的应用程序。只有在使用mmd -批评家构建分类器时，我们才有办法直接优化它。一种解决方案是屏幕绘图，在x轴上显示原型的数量，在y轴上显示MMD2度量。我们将选择MMD2曲线变平的原型的数量。

其他参数是内核和内核缩放参数的选择。我们遇到了与原型数量和批评数量相同的问题:我们如何选择内核及其扩展参数?同样，当我们使用mmd -批评家作为最接近的原型分类器时，我们可以调优内核参数。然而，对于无监督的mmd -批评家的用例，情况就不清楚了。(也许我在这里有点苛刻，因为所有的非监督方法都有这个问题。)
它将所有的特性作为输入，忽略了一些特性可能与预测感兴趣的结果无关的事实。一种解决方案是只使用相关的特性，例如图像嵌入而不是原始像素。只要我们能够将原始实例投射到只包含相关信息的表示上，这种方法就可以工作。

有一些可用的代码，但是它还没有被很好地打包成文档化的软件来实现。

6.3.5 规范和备选方案

在这里可以找到mmd -批评家的实现:https://github.com/beenkim/mmd -批评家。

寻找原型最简单的替代方法是k-medoids，由Kaufman et. al (1987).62

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image004.gif)

\1.    Kim、Be、Rajiv Khanna 和 Oluwasanmi O.Koyejo。“例子不够，要学会批判！对可解释性的批评〉，《神经信息处理系统的进展》（2016 年）。

\2.    考夫曼、伦纳德和彼得·卢梭。“通过类药物聚集”。北荷兰（1987）。

6.4 影响因素
机器学习模型最终是训练数据的产物，删除一个训练实例会影响结果模型。当从训练数据中删除一个训练实例而使模型的参数或预测发生较大变化时，我们称这个训练实例为“有影响的”。通过识别有影响的训练实例，我们可以“调试”机器学习模型，更好地解释它们的行为和预测。

本章向您展示了两种识别有影响实例的方法，即删除诊断和影响函数。这两种方法都基于稳健统计，稳健统计提供的统计方法受异常值或违反模型假设的影响较小。稳健统计还提供了度量来自数据的稳健估计的方法(例如平均值估计或预测模型的权重)。

假设你想估算你所在城市的人均收入，并随机询问街上的10个人他们的收入。除了你的样本可能真的很糟糕之外，你对平均收入的估计能在多大程度上受到一个人的影响?要回答这个问题，你可以通过省略个别答案来重新计算平均值，或者通过“影响函数”从数学上推导出平均值是如何被影响的。使用删除方法，我们重新计算平均值十次，每次删除一个损益表，并测量平均估计值的变化。一个大的变化意味着一个实例非常有影响力。第二种方法通过一个无穷小的权重来增加其中一个人的权重，这相当于统计量或模型的一阶导数的计算。这种方法也被称为“无穷小方法”或“影响函数”。顺便说一下，答案是，你的平均估计值会受到一个答案的强烈影响，因为平均值与单个值成线性关系。一个更可靠的选择是中位数(一半人赚得更多，另一半人赚得更少的值)，因为即使你的样本中收入最高的人赚得多十倍，得出的中位数也不会改变。
删除诊断和影响函数也可以应用于机器学习模型的参数或预测，以更好地理解它们的行为或解释单个预测。在查看这两种寻找有影响力实例的方法之前，我们将研究离群值和有影响力实例之间的区别。

离群值

离群值是与数据集中的其他实例相距甚远的实例。“Far away”是指所有其他实例之间的距离，例如欧几里德距离，非常大。在新生儿数据集中，一个新生儿重6公斤会被认为是一个异常值。在以支票账户为主的银行账户数据集中，专用贷款账户(大额负余额，很少交易)将被视为异常值。下图显示了一维分布的一个离群值。

图 6.14：特征 X 遵循高斯分布，X=8 处有一个异常值。

离群值可以是有趣的数据点（例如当异常值影响模型时，它也是一个有影响的实例。有影响的实例

一个有影响的实例是一个数据实例，它的删除对训练模型有很大的影响。

当模型被从训练数据中删除的特定实例重新训练时，模型参数或预测的更改越多，该实例的影响就越大。一个实例对训练模型是否有影响也取决于它对目标 Y 的值。下图显示了一个对线性回归模型有影响的实例。

图 6.15：具有一个特征的线性模型。培训一次完整的数据，一次没有影响的实例。删除影响的实例会显著改变拟合的坡度（重量/系数）。

为什么有影响的实例有助于理解模型？

对解释性有影响的实例背后的关键思想是跟踪模型参数和预测，使其回到最初的位置：培训数据。学习者，即生成机器学习模型的算法，是一个函数，它获取由特征 X 和目标 Y 组成的训练数据，并生成机器学习模型。例如，决策树的学习者是一种选择分割特征和分割值的算法。神经网络的学习者使用反向传播来找到最佳权重。

图 6.16：学习者从培训数据（功能和目标）中学习模型。该模型对新数据进行预测。

我们询问如果在训练过程中从训练数据中删除实例，模型参数或预测将如何变化。这与其他可解释性方法形成了对比，这些方法分析当我们操纵要预测的实例的特征(如部分依赖图或特征重要性)时预测是如何变化的。对于有影响的实例，我们不将模型视为固定的，而是将其视为训练数据的函数。有影响力的实例帮助我们回答有关全球模型行为和个人预测的问题。哪一个是对模型参数或总体预测影响最大的实例?对于一个特定的预测，哪些是最有影响力的例子?有影响的实例告诉我们模型在哪些实例上可能存在问题，哪些训练实例应该检查错误，并给我们一个模型健壮性的印象。如果一个单独的实例对模型的预测和参数有很大的影响，我们可能就不会相信模型了。至少这能让我们进一步调查。

我们如何找到有影响力的实例?我们有两种度量影响的方法:第一种方法是从训练数据中删除实例，在减少的训练数据集上重新训练模型，并观察模型参数或预测的差异(单独或整个数据集)。第二种选择是通过基于模型参数的梯度近似参数变化来增加数据实例的权重。删除方法更容易理解，并且激发了权重增加方法，所以我们从前者开始。

6.4.1 删除诊断

统计学家已经在有影响的实例领域做了大量的研究，特别是对于（广义）线性回归模型。当你搜索“有影响的观察结果”时，第一个搜索结果是关于 dfbeta 和 cook 距离等度量的。dfbeta 测量删除实例对模型参数的影响。Cook 的距离（Cook，197）测量删除实例对模型预测的影响。对于这两个度量，我们必须重复地重新培训模型，每次都忽略单个实例。将模型的所有实例的参数或预测与模型的参数或预测进行比较，其中一个实例已从训练数据中删除。

DFBETA 定义为：

\[dfbeta i=\beta-\beta ^（-i）\]

其中\（\beta\）是模型在所有数据实例上进行训练时的权重向量，而\（\beta^（-i）\）是模型在没有实例 i 的情况下进行训练时的权重向量。我会说非常直观。DFBETA 仅适用于具有权重参数的模型，如逻辑回归或神经网络，但不适用于决策树、树集合、某些支持向量机等模型。

Cook 距离是为线性回归模型而发明的，并且存在广义线性回归模型的近似值。Cook 对训练实例的距离定义为当第 i 个实例从模型训练中移除时，预测结果的平方差之和。

\[d i=\frac \ sum j=1 ^n（\hat y（-i））^2 p\cdot mse]

其中，分子是有第 i 个实例和没有第 i 个实例的模型预测之间的平方差，在数据集上求和。分母是特征数 p 乘以均方误差。无论删除哪个实例，所有实例的分母都相同。库克的距离告诉我们，当我们从训练中移除第 i 个实例时，线性模型的预测输出会发生多大的变化。

我们能把库克的距离和 dfbeta 用于任何机器学习模型吗？dfbeta 需要模型参数，因此此度量仅适用于参数化模型。库克的距离不需要任何模型参数。有趣的是，Cook 的距离通常不在线性模型和广义线性模型的范围之外，但考虑到特定实例删除前后模型预测之间的差异的想法是非常普遍的。库克距离定义的一个问题是 MSE，这对所有类型的预测模型（如分类）都没有意义。

对模型预测影响的最简单影响测量可写如下：

\[\text 影响^（-i）=\frac

这个表达式基本上是库克距离的分子，用绝对差代替平方差相加。这是我做的一个选择，因为它对后面的例子有意义。删除诊断度量的一般形式包括选择一个度量（如预测结果），并计算在所有实例上训练的模型和删除实例时度量的差异。

我们可以很容易地将影响分解，为实例 j 的预测解释第 i 次培训实例的影响是什么：

\[\文本影响（-i）=\左\右（-i）\右]

这也适用于模型参数的差异或损失的差异。在下面的示例中，我们将使用这些简单的影响度量。

## 删除诊断示例

在下面的例子中，我们训练一个支持向量机来预测给定的风险因素，并测量哪些训练实例对整体和特定的预测最有影响。由于癌症预测是一个分类问题，因此我们将影响作为癌症预测概率的差异来衡量。当实例从模型训练中移除时，如果预测的概率在数据集中平均显著增加或减少，则实例具有影响。对所有 858 个培训实例的影响测量需要对所有数据进行一次模型培训，并在每次删除其中一个实例的情况下对模型进行 858 次重新培训（=培训数据的大小）。

最有影响的实例的影响度量约为 0.01。0.01 的影响意味着如果我们去掉第 540 个实例，预测的概率平均变化 1 个百分点。考虑到癌症的平均预测概率为 6.4%，这是相当可观的。所有可能删除的影响度量的平均值为 0.2 个百分点。现在我们知道哪些数据实例对模型最有影响。这对于调试数据已经很有用了。是否存在问题实例？是否存在测量误差？有影响的实例是第一个应该检查错误的实例，因为其中的每个错误都强烈影响模型预测。

除了模型调试之外，我们可以学习一些东西来更好地理解模型吗？仅仅打印出前 10 个最有影响力的实例并不是很有用，因为它只是一个具有许多特性的实例表。所有将实例作为输出返回的方法只有在我们有好的方法来表示它们的情况下才有意义。但是，当我们问：有影响力的实例和没有影响力的实例有什么区别时，我们可以更好地理解哪些实例是有影响力的？我们可以将这个问题转化为回归问题，并将实例的影响建模为其特征值的函数。我们可以从本章中自由选择任何型号。对于这个例子，我选择了一个决策树（下图），它显示来自 35 岁及以上妇女的数据对支持向量机最有影响。在数据集中的所有女性中，858 人中有 153 人年龄大于 35 岁。在这一章中，我们已经看到，40 岁以后，癌症的预测概率急剧增加，而且年龄也是最重要的特征之一。影响分析告诉我们，当预测癌症的年龄更高时，模型变得越来越不稳定。这本身就是有价值的信息。这意味着这些实例中的错误会对模型产生很大的影响。

图 6.17：模拟实例影响与其特征之间关系的决策树。树的最大深度设置为 2。

这第一个影响分析揭示了最具影响力的总体情况。现在我们选择其中一个实例，即第 7 个实例，我们想通过找到最有影响的训练数据实例来解释预测。这就像一个反事实的问题：如果我们从培训过程中忽略了实例 I，那么实例 7 的预测会发生什么变化？我们对所有实例重复此删除。然后，我们选择在训练中忽略实例 7 时，导致预测最大变化的训练实例，并用它们来解释该实例的模型预测。我选择解释这个预测，例如 7，因为它是癌症预测概率最高的实例（7.35%），我认为这是一个有趣的案例，需要更深入地分析。我们可以返回前 10 个最有影响力的实例来预测作为表格打印的第 7 个实例。不是很有用，因为我们看不到太多。再次，通过分析影响实例和非影响实例的特点，找出影响实例和非影响实例之间的区别是更有意义的。我们使用一个经过训练的决策树来预测给定特征的影响，但实际上我们滥用它只是为了找到一个结构，而不是实际预测某些东西。下面的决策树显示了哪种训练实例对预测第 7 个实例最有影响。

图 6.18：解释哪些实例对预测第 7 个实例最有影响的决策树。吸烟 18.5 年或更长时间的妇女的数据对第 7 例的预测有很大影响，绝对预测的平均变化为癌症概率的 11.7 个百分点。

吸烟或已吸烟 18.5 年或更长时间的妇女的数据实例对第 7 例的预测有很大影响。第 7 例背后的女人吸烟了 34 年。数据显示，12 名女性（1.40%）吸烟 18.5 年或更长时间。在收集其中一名妇女的吸烟年限时所犯的任何错误都将对第 7 例的预测结果产生巨大影响。

当我们删除实例 663 时，预测会发生最极端的变化。该患者据称吸烟 22 年，与决策树的结果一致。如果删除实例 663，第 7 个实例的预测概率从 7.35%变为 66.60%。

如果我们仔细看看最有影响的实例的特性，我们可以看到另一个可能的问题。数据显示，这名妇女 28 岁，已经吸烟 22 年了。要么是非常极端的情况，她 6 岁就开始抽烟，要么就是数据错误。我倾向于相信后者。在这种情况下，我们必须质疑数据的准确性。

这些示例显示了识别对调试模型有影响的实例是多么有用。该方法的一个问题是，模型需要针对每个训练实例进行重新训练。整个再培训过程可能非常缓慢，因为如果您有数千个培训实例，则必须对模型进行数千次再培训。假设模型需要一天的训练时间，并且您有 1000 个训练实例，那么计算有影响的实例（没有并行化）将需要近 3 年的时间。没人有时间做这个。在本章的其余部分中，我将向您展示一种不需要重新培训模型的方法。

6.4.2 影响功能

你：我想知道一个训练实例对特定预测的影响。

研究：您可以删除培训实例，重新培训模型，并测量预测的差异。

你：太好了！但是你有没有一个方法可以让我不用再培训？这需要很多时间。研究：你有一个损失函数的模型，它的参数是两倍可微的吗？

你：我训练了一个神经网络来处理物流损失。所以是的。

研究：然后用影响函数来近似实例对模型参数和预测的影响。影响函数是衡量模型参数或预测对训练实例的依赖程度的指标。方法不是删除实例，而是通过一个非常小的步骤对丢失的实例进行加权。该方法包括利用梯度和黑森矩阵近似当前模型参数的损失。损失增加权重类似于删除实例。你：太好了，这就是我要找的！

Koh 和 Liang（2017 年建议使用影响函数（一种稳健统计方法）来测量实例如何影响模型参数或预测。与删除诊断一样，影响函数跟踪模型参数和预测返回到负责的培训实例。然而，该方法并没有删除训练实例，而是近似于当实例在经验风险（训练数据损失之和）中被加权时，模型的变化程度。

影响函数的方法要求访问与模型参数相关的损失梯度，这只适用于机器学习模型的一个子集。逻辑回归、神经网络和支持向量机是合格的，像随机森林这样的基于树的方法是不合格的。影响函数有助于理解模型行为、调试模型和检测数据集中的错误。

以下部分解释了影响函数背后的直觉和数学。

## 影响函数背后的数学

影响函数背后的关键思想是通过无限小的步骤（epsilon）增加训练实例的损失，从而产生新的模型参数：

\[\hat \theta \epsilon，z=\arg \min \theta \i n \theta（1-\epsilon \frac n \sum i=1 l（z i，theta）

+\ epsilon l（z，theta）\]

其中\（\theta\）是模型参数向量，而\（\hat \ theta \ epsilon，z \）是用非常小的数字\（\epsilon\）对 z 进行加权后的参数向量。l 是模型训练的损失函数，z 是训练数据，z 是我们想要增加权重来模拟其移除的训练实例。这个公式背后的直觉是：如果我们将训练数据中的某个特定实例（z_i）稍微增加一点（\（\ epsilon\）并相应地降低其他数据实例的权重，损失会有多大变化？参数向量是什么样子来优化这个新的组合损失？参数的影响函数，即上权训练实例 Z 对参数的影响，可计算如下。

\[I   \ Theta \\ \ Epsilon，Z \123\ \ \\123;  123; \θl（z，that \θ）\]

最后一个表达式\（\nabla \theta l（z，that \theta）是相对于上权训练实例参数的损失梯度。梯度是训练实例丢失的变化率。它告诉我们，当我们稍微改变模型参数时，损失会有多大的变化。梯度向量中的正项意味着相应模型参数的微小增加会增加损失，负项意味着参数的增加会减少损失。第一部分\（h^-1 \ that \ theta）是逆 Hessian 矩阵（损失相对于模型参数的二阶导数）。黑森矩阵是梯度变化率，或表示为损耗，它是损耗变化率的变化率。可以使用下列公式进行估算：

更非正式的是：黑森矩阵记录了损失在某一点上的弯曲程度。Hessian 是一个矩阵，而不仅仅是一个向量，因为它描述了损失的曲率，曲率取决于我们观察的方向。如果你有许多参数，那么海森矩阵的实际计算是耗时的。Koh 和 Liang 提出了一些有效计算的技巧，这超出了本章的范围。如上述公式所述，更新模型参数，相当于在估计模型参数周围形成二次展开后采取一个牛顿步骤。

这个影响函数公式背后的直觉是什么？公式来自于围绕参数形成二次展开式。这意味着我们实际上不知道，或者它太复杂了，无法计算实例 z 的损失在被删除/加权时究竟会有多大的变化。我们利用当前模型参数设置下的陡度（=梯度）和曲率（=黑森矩阵）的信息对函数进行局部近似。通过这种损失近似，我们可以计算出如果我们对实例 z 进行加权，新参数将大致是什么样子：

\[\Hat \Theta-Z \约\Hat \Theta-\Frac 1 n i \ Text up，参数（Z）]

近似参数向量基本上是原始参数减去 z 的损失梯度（因为我们想减少损失），用曲率（=乘以逆 Hessian 矩阵）缩放，用 1 除以 n 缩放，因为这是单个训练实例的权重。

下图显示了向上加权的工作原理。x 轴显示了\（\theta\）参数的值，y 轴显示了加权实例 z 对应的损失值。这里的模型参数是一维的，用于演示，但实际上通常是高维的。我们只移动 1 到 n 的方向上，以改善损失，例如 z。我们不知道当我们删除 z 时损失会如何变化，但是使用损失的一阶和二阶导数，我们围绕当前模型参数创建这个二次近似，并假设真正的损失是如何表现的。

图 6.19：通过在当前模型参数周围形成损耗的二次扩展，并将 1/N 移动到具有上加权实例 z（y 轴）的损耗改善最多的方向，更新模型参数（x 轴）。如果我们删除 Z 并在减少的数据上对模型进行训练，则损失中实例 Z 的上浮近似于参数变化。

我们实际上不需要计算新的参数，但是我们可以使用影响函数来测量 z 对参数的影响。

当我们增加训练实例 z 的权重时，预测是如何变化的？我们既可以计算新参数，然后使用新参数化模型进行预测，也可以直接计算实例 z 对预测的影响，因为我们可以使用链规则计算影响：

\[\begin align*i up，loss（z，z test）&=\ left.\frac d l（z test，\hat \theta \epsilon，z）d \epsilon

\右\ epsilon=0 \ \左.\nabla \ theta l（z 测试，\t hat \ theta）^t\frac d\that \ theta \ epsilon，z

；）\结束对齐*\]

这个方程的第一行意味着我们测量训练实例对某个预测（z_test \）的影响，当我们增加实例 z 的权重并获得新参数时，作为测试实例损失的变化。对于方程的第二行，我们应用了导数链规则，得到了测试实例损失的导数，它与参数乘以 z 对参数的影响。在第三行，我们将表达式替换为参数的影响函数。第三行中的第一个术语是测试实例相对于模型参数的梯度。

有一个公式是伟大的，科学和准确的方式显示事物。但我认为对公式的含义有一些直觉是非常重要的。（i_ text up，loss）的公式指出，训练实例 z 对实例预测的影响函数为“当我们对 inst 进行权重计算时，实例对模型参数变化的反应程度”乘以“参数变化的程度”。安斯 Z“。阅读公式的另一种方法是：影响与训练和测试损失的梯度大小成正比。训练损失梯度越大，对参数的影响越大，对测试预测的影响越大。测试预测的梯度越高，对测试实例的影响就越大。整个结构也可以看作是训练和测试实例之间相似性的度量（由模型学习）。

这就是理论和直觉。下一节将解释如何应用影响函数。

## 影响函数的应用

影响函数有许多应用，其中一些已经在本章中介绍。

## 了解模型行为

不同的机器学习模型有不同的预测方法。即使两个模型具有相同的性能，它们根据特性进行预测的方式也可能非常不同，因此在不同的场景中会失败。通过识别有影响力的实例来理解模型的特定弱点，有助于在您的头脑中形成机器学习模型行为的“心智模型”。下图展示了一个例子，一个支持向量机(SVM)和一个神经网络被训练来区分狗和鱼的图像。对这两种模型来说，最具影响力的鱼的典型形象是非常不同的。对于支持向量机，如果实例在颜色上相似，则它们具有影响力。对于神经网络，如果它们在概念上相似，实例就会产生影响。对于神经网络来说，即使是一张狗的图片也是最具影响力的图片之一，这表明它在颜色空间中学习的是概念而不是欧氏距离。

图 6.20：狗还是鱼？对于与测试图像颜色相似的支持向量机预测（中行）图像，影响最大。对于神经网络预测（底排），不同环境下的鱼类影响最大，但也有狗的图像（右上角）。Koh 和 Liang 的作品（2017 年）。

## 处理域不匹配/调试模型错误

处理域不匹配与更好地理解模型行为密切相关。领域失配是指训练数据和测试数据的分布不一致，从而导致模型在测试数据上表现不佳。影响函数可以识别导致错误的训练实例。假设你训练了一个预测模型来预测接受手术的病人的结果。所有这些病人都来自同一家医院。现在你在另一家医院使用这个模型，你会发现它对很多病人都不起作用。当然，你假设这两家医院有不同的病人，如果你看他们的数据，你可以看到他们在许多特征上是不同的。但是什么特性或实例“破坏”了模型呢?在这里，有影响力的例子也是回答这个问题的好方法。您以一个新患者为例，该模型对其做出了错误的预测，然后找到并分析最有影响力的实例。例如，这可能表明，第二医院平均有老年患者，从训练数据中最具影响力的实例是来自第一医院的少数老年患者，而该模型只是缺乏数据来学习预测这个亚组。结论是，该模型需要在更多的老年患者身上进行培训，以便在第二医院更好地工作。

## 修复培训数据

如果您对检查正确性的训练实例的数量有限制，那么如何进行有效的选择呢?最好的方法是选择最有影响力的实例，因为根据定义，它们对模型的影响最大。即使您有一个具有明显不正确值的实例，如果实例不具有影响力，并且您只需要预测模型的数据，那么检查有影响力的实例是更好的选择。例如，您训练一个模型来预测病人应该继续住院还是提前出院。你真的想要确保这个模型是健壮的并且做出正确的预测，因为一个错误的病人释放可能会有不好的后果。病人的记录可能非常混乱，所以你对数据的质量没有完全的信心。但检查患者信息和纠正它可能非常耗时,因为一旦你报道哪些病人需要检查,医院实际需要派人查看选中的记录病人更密切,这可能是手写的,躺在一些档案。为病人检查数据可能需要一个小时或更长时间。考虑到这些成本，只检查几个重要的数据实例是有意义的。最好的方法是选择对预测模型有较大影响的患者。Koh和Liang(2017)表明，这种类型的选择比随机选择或选择那些损失最大或分类错误的选择要好得多。

6.4.3 识别有影响的实例的优势

删除诊断和影响函数的方法与模型无关章节中介绍的基于特征扰动的方法非常不同。有影响力的例子强调了培训数据在学习过程中的作用。这使得影响函数和删除诊断成为机器学习模型的最佳调试工具之一。在本书介绍的技术中，它们是唯一直接帮助识别应该检查错误的实例的技术。

删除诊断与模型无关，这意味着该方法可以应用于任何模型。基于导数的影响函数也可应用于一类广泛的模型。

我们可以使用这些方法来比较不同的机器学习模型，更好地理解它们的不同行为，而不仅仅是比较预测性能。

本章我们没有讨论这个话题，但是通过导数的影响函数也可以用来创建对抗训练数据。这些实例以这样一种方式被操纵，当模型被训练在那些被操纵的实例上时，模型不能正确地预测某些测试实例。对抗性攻击的不同之处在于，攻击发生在训练期间，也称为投毒攻击。如果你感兴趣，请阅读Koh和Liang(2017)的论文。

对于缺失诊断和影响函数，我们考虑了预测的差异，对于影响函数，我们考虑了损失的增加。但是，实际上，这种方法适用于表单的任何问题:“当我们删除或增加实例z的权重时……会发生什么?”，你可以用你的欲望模型的任何功能来填充“…”。您可以分析一个训练实例在多大程度上影响了模型的总体损失。您可以分析培训实例对特性重要性的影响程度。您可以分析在训练决策树时，训练实例对第一次分割所选择的特性的影响有多大。

6.4.4 识别有影响的实例的缺点

删除诊断的计算非常昂贵，因为它们需要重新培训。但历史表明，计算机资源在不断增加。一项20年前在资源方面难以想象的计算可以很容易地在你的智能手机上完成。您可以在几秒/分钟内在笔记本电脑上使用数千个训练实例和数百个参数来训练模型。因此，假定删除诊断在10年内即使使用大型神经网络也不会出现问题，这并不是一个很大的飞跃。

影响函数是一种很好的删除诊断方法，但只适用于参数可微的模型，如神经网络。它们并不适用于基于树木的方法，如随机森林、增强树或决策树。即使您有带有参数和损失函数的模型，损失也可能是不可微的。但对于最后一个问题，有一个技巧:使用可微损耗来代替计算影响，例如，当基础模型使用铰链损耗而不是一些可微损耗时。对影响函数的损失进行平滑化处理来代替损失，但仍然可以用非平滑损失来训练模型。

影响函数只是近似的，因为该方法围绕参数形成二次展开。这种近似值可能是错误的，当移除实例时，实例的影响实际上会更高或更低。Koh和Liang(2017)在一些例子中表明，通过影响函数计算出的影响与实际删除实例后对模型进行再训练时所得到的影响测度接近。但不能保证近似总是如此接近。
我们称一个实例为有影响的或无影响的影响度量没有明确的截止日期。按影响力对实例进行排序是有用的，但如果不仅能够对实例进行排序，而且能够实际区分有影响力和无影响力的实例，那就太好了。例如，如果您为一个测试实例确定了10个最具影响力的培训实例，其中一些可能并不具有影响力，因为，例如，只有前3个实例具有真正的影响力。

影响度量只考虑单个实例的删除，而不是同时删除多个实例。较大的数据实例组可能具有一些交互作用，这些交互作用强烈影响模型的训练和预测。但问题在于组合方法:从数据中删除单个实例有n种可能。从训练数据中删除两个实例有n次(n-1)的可能性。有n次(n-1) (n-2)次(n-2)次删除三次的可能性…我想你可以看到这是怎么回事，有太多的组合了。

6.4.5 软件和备选方案

删除诊断非常容易实现。看一下我为本章的示例编写的代码。

对于线性模型和广义线性模型，在stats包的R中实现了许多影响度量，如Cook’s distance。

Koh和Liang在他们的论文中发表了影响函数的Python代码。这是伟大的!不幸的是，它只是论文的代码，而不是一个维护和文档化的Python模块。代码主要关注于Tensorflow库，因此不能直接将其用于使用其他框架(如sci-kit learn)的黑盒模型。
Keita Kurita写了一篇关于影响函数的博文，帮助我更好地理解了Koh和梁的论文。这篇博客文章对黑箱模型影响函数背后的数学原理进行了更深入的探讨，并讨论了一些数学“技巧”，从而有效地实现了这种方法。

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image001.gif)

\1.    库克，R.丹尼斯。“线性回归中影响观察的检测”，《技术计量学》19.1（1977）：15-18。

\2.    Koh、庞伟和 Percy Liang。“通过影响函数了解黑盒预测”，arxiv 预印 arxiv:1703.04730（2017）。

